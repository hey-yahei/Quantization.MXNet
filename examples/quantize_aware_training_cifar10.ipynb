{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MXNet package\n",
    "from mxnet import nd, init, cpu, gpu, gluon, autograd\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data import DataLoader, Sampler\n",
    "from mxnet.gluon.data.vision import CIFAR10, transforms as T\n",
    "from gluoncv.data import transforms as gcv_T\n",
    "from gluoncv.model_zoo import cifar_resnet56_v1\n",
    "\n",
    "# Normal package\n",
    "import time\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "# Custom package\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from quantize import convert\n",
    "from quantize.initialize import qparams_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    # Model\n",
    "    num_class = 10\n",
    "    \n",
    "    # Train\n",
    "    max_steps = 8000\n",
    "    train_batch_size = 64\n",
    "    val_batch_size = 128\n",
    "    train_num_workers = 4\n",
    "    val_num_workers = 4\n",
    "    lr = 1e-6\n",
    "    \n",
    "    # Record\n",
    "    ckpt_dir = \"./tmp/checkpoints\"\n",
    "    main_tag = 'cifar_resnet56_v1_quantize'\n",
    "    ckpt_prefix = 'cifar_resnet56_v1_quantize'\n",
    "    train_record_per_steps = 200\n",
    "    val_per_steps = 400\n",
    "    spotter_starts_at = 10000\n",
    "    spotter_window_size = 10\n",
    "    patience = 20\n",
    "    snapshot_per_steps = 400\n",
    "    \n",
    "    # Quantize\n",
    "    offline_at = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(Config.ckpt_dir):\n",
    "    os.mkdir(Config.ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_stamp = time.strftime('%Y%m%d_%H%M%S',time.localtime(time.time()))\n",
    "writer = SummaryWriter(log_dir=\"tmp/runs/{}_{}\".format(Config.main_tag, datetime_stamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize inputs\n",
    "converter = {\n",
    "    nn.Conv2D: convert.gen_conv2d_converter(quant_type=\"channel\", fake_bn=True, input_width=4, weight_width=4),\n",
    "    nn.Dense: convert.gen_dense_converter(quant_type=\"channel\"),\n",
    "    nn.Activation: None,\n",
    "    nn.BatchNorm: convert.bypass_bn\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cifar_resnet56_v1(pretrained=True)\n",
    "convert.convert_model(net, exclude=[net.features[0], net.features[1]], convert_fn=converter)\n",
    "net.quantize_input(enable=False)\n",
    "qparams_init(net)\n",
    "net.collect_params().reset_ctx(gpu(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, num_class, dataloader, ctx):\n",
    "    t = time.time()\n",
    "    correct_counter = nd.zeros(num_class)\n",
    "    label_counter = nd.zeros(num_class)\n",
    "    test_num_correct = 0\n",
    "    eval_loss = 0.\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        X = X.as_in_context(ctx)\n",
    "        y = y.as_in_context(ctx)\n",
    "\n",
    "        outputs = net(X)\n",
    "        loss = loss_func(outputs, y)\n",
    "        eval_loss += loss.sum().asscalar()\n",
    "        pred = outputs.argmax(axis=1)\n",
    "        test_num_correct += (pred == y.astype('float32')).sum().asscalar()\n",
    "\n",
    "        pred = pred.as_in_context(cpu())\n",
    "        y = y.astype('float32').as_in_context(cpu())\n",
    "        for p, gt in zip(pred, y):\n",
    "            label_counter[gt] += 1\n",
    "            if p == gt:\n",
    "                correct_counter[gt] += 1\n",
    "\n",
    "    eval_loss /= len(test_dataset)\n",
    "    eval_acc = test_num_correct / len(test_dataset)\n",
    "    eval_acc_avg = (correct_counter / (label_counter+1e-10)).mean().asscalar()\n",
    "    \n",
    "    return eval_loss, eval_acc, eval_acc_avg, time.time()-t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformer = T.Compose([\n",
    "    gcv_T.RandomCrop(32, pad=4),\n",
    "    T.RandomFlipLeftRight(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "eval_transformer = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CIFAR10(train=True).transform_first(train_transformer)\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=Config.train_batch_size,\n",
    "                          num_workers=Config.train_num_workers,\n",
    "                          last_batch='discard')\n",
    "test_dataset = CIFAR10(train=False).transform_first(eval_transformer)\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=Config.val_batch_size, \n",
    "                         shuffle=False,\n",
    "                         num_workers=Config.val_num_workers, \n",
    "                         last_batch='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "train_size = len(train_dataset)\n",
    "val_size = len(test_dataset)\n",
    "print(f'trainset size => {train_size}')\n",
    "print(f'valset size => {val_size}')\n",
    "steps_per_epoch = train_size / Config.train_batch_size\n",
    "print(f'{steps_per_epoch} steps for per epoch (BATCH_SIZE={Config.train_batch_size})')\n",
    "print(\"record per {} steps ({} samples, {} times per epoch)\".format(\n",
    "                                                            Config.train_record_per_steps,\n",
    "                                                            Config.train_record_per_steps * Config.train_batch_size,\n",
    "                                                            steps_per_epoch / Config.train_record_per_steps))\n",
    "print(\"evaluate per {} steps ({} times per epoch)\".format(\n",
    "                                                    Config.val_per_steps,\n",
    "                                                    steps_per_epoch / Config.val_per_steps))\n",
    "print(\"spotter start at {} steps ({} epoches)\".format(\n",
    "                                                Config.spotter_starts_at,\n",
    "                                                Config.spotter_starts_at / steps_per_epoch))\n",
    "print(\"size of spotter window is {} ({} steps)\".format(\n",
    "                                                Config.spotter_window_size,\n",
    "                                                Config.spotter_window_size * Config.val_per_steps))\n",
    "print(\"max patience: {} ({} steps; {} samples; {} epoches)\".format(\n",
    "                                                            Config.patience,\n",
    "                                                            Config.patience * Config.val_per_steps,\n",
    "                                                            Config.patience * Config.val_per_steps * Config.train_batch_size,\n",
    "                                                            Config.patience * Config.val_per_steps / steps_per_epoch))\n",
    "print(\"snapshot per {} steps ({} times per epoch)\".format(\n",
    "                                                    Config.snapshot_per_steps,\n",
    "                                                    steps_per_epoch / Config.snapshot_per_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_steps = 0\n",
    "good_acc_window = [0.] * Config.spotter_window_size\n",
    "estop_loss_window = [0.] * Config.patience\n",
    "quantize_offline = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': Config.lr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Evaluate\n",
    "eval_loss, eval_acc, eval_acc_avg, cost_time = evaluate(net, Config.num_class, test_loader, ctx=gpu(0))\n",
    "writer.add_scalars(f'{Config.main_tag}/Loss', {'val': eval_loss}, global_steps)\n",
    "writer.add_scalars(f'{Config.main_tag}/Acc', {\n",
    "    'val': eval_acc,\n",
    "    'val_avg': eval_acc_avg\n",
    "}, global_steps)\n",
    "print(f\"Evaluate cost time: {cost_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "size_per_record = Config.train_record_per_steps * Config.train_batch_size\n",
    "flag_early_stop = False\n",
    "train_loss = 0.\n",
    "train_num_correct = 0\n",
    "prune_counter = 0\n",
    "t = time.time()\n",
    "while global_steps < Config.max_steps and not flag_early_stop:\n",
    "    for X, y in train_loader:\n",
    "        global_steps += 1\n",
    "        # Move data to gpu\n",
    "        X = X.as_in_context(gpu(0))\n",
    "        y = y.as_in_context(gpu(0))\n",
    "        # Forward & Backward\n",
    "        with autograd.record():\n",
    "            outputs = net(X)\n",
    "            loss = loss_func(outputs, y)\n",
    "        net.update_ema()\n",
    "        loss.backward()\n",
    "#         trainer.step(Config.train_batch_size)\n",
    "        trainer.step(Config.train_batch_size, ignore_stale_grad=True)     # if bypass bn\n",
    "        \n",
    "        train_loss += loss.sum().asscalar()\n",
    "        pred = outputs.argmax(axis=1)\n",
    "        train_num_correct += (pred == y.astype('float32')).sum().asscalar()\n",
    "        \n",
    "        # Record training info\n",
    "        if global_steps and global_steps % Config.train_record_per_steps == 0:\n",
    "            writer.add_scalars(f'{Config.main_tag}/Loss', {'train': train_loss/size_per_record}, global_steps)\n",
    "            writer.add_scalars(f'{Config.main_tag}/Acc', {'train': train_num_correct/size_per_record}, global_steps)\n",
    "            train_loss = 0.\n",
    "            train_num_correct = 0\n",
    "            \n",
    "        # Evaluate\n",
    "        if global_steps and global_steps % Config.val_per_steps == 0:\n",
    "            # Quantize\n",
    "            if not quantize_offline and global_steps >= Config.offline_at:\n",
    "                print(\"Quantize offline...\")\n",
    "                net.quantize_input(enable=True, online=False)\n",
    "                quantize_offline = True\n",
    "            \n",
    "            # Evaluate\n",
    "            eval_loss, eval_acc, eval_acc_avg, __ = evaluate(net, Config.num_class, test_loader, ctx=gpu(0))\n",
    "            writer.add_scalar(f'{Config.main_tag}/Speed', Config.val_per_steps / (time.time() - t), global_steps)\n",
    "            writer.add_scalars(f'{Config.main_tag}/Loss', {'val': eval_loss}, global_steps)\n",
    "            writer.add_scalars(f'{Config.main_tag}/Acc', {\n",
    "                'val': eval_acc,\n",
    "                'val_avg': eval_acc_avg\n",
    "            }, global_steps)\n",
    "            \n",
    "            # Spotter\n",
    "            good_acc_window.pop(0)\n",
    "            if global_steps >= Config.spotter_starts_at and eval_acc > max(good_acc_window):\n",
    "                print( \"catch a good model with acc {:.6f} at {} step\".format(eval_acc, global_steps) )\n",
    "                writer.add_text(Config.main_tag, \"catch a good model with acc {:.6f}\".format(eval_acc), global_steps)\n",
    "                net.save_parameters(\"{}/{}-{:06d}.params\".format(Config.ckpt_dir, Config.ckpt_prefix, global_steps))\n",
    "            good_acc_window.append(eval_acc)\n",
    "\n",
    "            # Early stop\n",
    "            estop_loss_window.pop(0)\n",
    "            estop_loss_window.append(eval_loss)\n",
    "            if global_steps > Config.val_per_steps * len(estop_loss_window):\n",
    "                min_index = estop_loss_window.index( min(estop_loss_window) )\n",
    "                writer.add_scalar(f'{Config.main_tag}/val/Patience', min_index, global_steps)\n",
    "                if min_index == 0:\n",
    "                    flag_early_stop = True\n",
    "                    print(\"early stop at {} steps\".format(global_steps))\n",
    "                    break\n",
    "            \n",
    "            t = time.time()\n",
    "        \n",
    "        # Snapshot\n",
    "        if global_steps and global_steps % Config.snapshot_per_steps == 0:\n",
    "            net.save_parameters(\"{}/{}-{:06d}.params\".format(Config.ckpt_dir, Config.ckpt_prefix, global_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
